import streamlit as st
import requests
from bs4 import BeautifulSoup
import urllib.parse

# ì´ì§‘íŠ¸ ì£¼ìš” ë„ì‹œ ë¦¬ìŠ¤íŠ¸
EGYPT_CITIES = [
    "ì¹´ì´ë¡œ ë‚ ì”¨", 
    "ë£©ì†Œë¥´ ë‚ ì”¨", 
    "ì•„ìŠ¤ì™„ ë‚ ì”¨", 
    "í›„ë¥´ê°€ë‹¤ ë‚ ì”¨", 
    "ì•Œë ‰ì‚°ë“œë¦¬ì•„ ë‚ ì”¨"
]

def get_weather_data_from_google(city_query):
    """
    Google ê²€ìƒ‰ ê²°ê³¼ë¥¼ ìŠ¤í¬ë˜í•‘í•˜ì—¬ ë‚ ì”¨ ë°ì´í„°ë¥¼ ê°€ì ¸ì˜¤ëŠ” í•¨ìˆ˜
    """
    # Google ê²€ìƒ‰ URL ìƒì„±
    # ì¿¼ë¦¬ ë¬¸ìì—´ì„ URL ì¸ì½”ë”©í•˜ì—¬ í•œê¸€ ê²€ìƒ‰ì„ ì§€ì›í•©ë‹ˆë‹¤.
    encoded_query = urllib.parse.quote(city_query)
    url = f"https://www.google.com/search?q={encoded_query}"
    
    # ì›¹ ìŠ¤í¬ë˜í•‘ ì‹œ ë´‡ìœ¼ë¡œ ì¸ì‹ë˜ì§€ ì•Šë„ë¡ User-Agent ì„¤ì •
    headers = {
        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'
    }
    
    try:
        response = requests.get(url, headers=headers, timeout=10)
        response.raise_for_status() # HTTP ì˜¤ë¥˜ ë°œìƒ ì‹œ ì˜ˆì™¸ ë°œìƒ
        
        soup = BeautifulSoup(response.content, 'html.parser')
        
        # Google ë‚ ì”¨ ìœ„ì ¯ì—ì„œ ë°ì´í„°ë¥¼ ì¶”ì¶œí•©ë‹ˆë‹¤.
        # ì´ ì„ íƒì(Selector)ëŠ” Googleì˜ HTML êµ¬ì¡°ì— ë”°ë¼ ë‹¬ë¼ì§ˆ ìˆ˜ ìˆìŠµë‹ˆë‹¤.
        location = soup.find("div", id="wob_loc")
        time_desc = soup.find("div", id="wob_dts") # ì‹œê°„ê³¼ ë‚ ì”¨ ì„¤ëª…
        temp_div = soup.find("span", id="wob_tm") # í˜„ì¬ ì˜¨ë„
        
        # ì¶”ê°€ ì •ë³´ ì¶”ì¶œ (ìŠµë„, í’ì† ë“±)
        # ì´ ì •ë³´ëŠ” ë‹¤ë¥¸ ìœ„ì¹˜ì— ìˆì„ ìˆ˜ ìˆìœ¼ë©°, IDë¡œ ì§ì ‘ ì°¾ê¸° ì–´ë ¤ìš¸ ìˆ˜ ìˆìŠµë‹ˆë‹¤.
        # ì¼ë°˜ì ì¸ div í´ë˜ìŠ¤ì—ì„œ ì°¾ë„ë¡ ì‹œë„í•©ë‹ˆë‹¤.
        
        # "ê°•ìˆ˜ëŸ‰", "ìŠµë„", "í’ì†"ì„ í¬í•¨í•˜ëŠ” ì»¨í…Œì´ë„ˆ
        # *ì£¼ì˜*: ì´ í´ë˜ìŠ¤ ì„ íƒìëŠ” ê°€ì¥ ë¶ˆì•ˆì •í•œ ë¶€ë¶„ì…ë‹ˆë‹¤.
        details_container = soup.find("div", class_="wob_gbox") 
        
        details = {}
        if details_container:
            # key-value ìŒì„ ì°¾ê¸° (ì˜ˆ: 'ê°•ìˆ˜ëŸ‰: 1%', 'ìŠµë„: 70%', 'í’ì†: 15 km/h')
            for item in details_container.find_all('div', class_="wtsrph"):
                key_elements = item.find_all('span', class_="wob_tbu")
                value_elements = item.find_all('span')
                
                if len(key_elements) >= 1 and len(value_elements) >= 2:
                    key = key_elements[0].text.strip()
                    # ë‘ ë²ˆì§¸ span ìš”ì†Œê°€ ê°’ì— í•´ë‹¹í•˜ëŠ” ê²½ìš°ê°€ ë§ìŠµë‹ˆë‹¤.
                    value = value_elements[1].text.strip()
                    details[key] = value

        
        # ì¶”ì¶œëœ ë°ì´í„°ê°€ ìœ íš¨í•œì§€ í™•ì¸í•˜ê³  ë°˜í™˜
        if location and temp_div:
            # ì¶”ì¶œëœ í…ìŠ¤íŠ¸ì—ì„œ ë¶ˆí•„ìš”í•œ ê³µë°± ì œê±°
            location_text = location.text.strip()
            temp_text = temp_div.text.strip()
            time_desc_text = time_desc.text.strip() if time_desc else 'N/A'
            
            # ë‚ ì”¨ ìƒíƒœ ì¶”ì¶œ (ex: ë§‘ìŒ, íë¦¼)
            weather_status_span = soup.find("span", id="wob_dc")
            weather_status = weather_status_span.text.strip() if weather_status_span else 'N/A'
            
            return {
                "location": location_text,
                "current_time_desc": time_desc_text,
                "temperature": f"{temp_text} Â°C",
                "status": weather_status,
                "details": details
            }
        else:
            return None
            
    except requests.exceptions.RequestException as e:
        st.error(f"ì›¹ ìš”ì²­ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
        return None
    except Exception as e:
        # ìŠ¤í¬ë˜í•‘ ì‹¤íŒ¨ (HTML êµ¬ì¡° ë³€ê²½ ê°€ëŠ¥ì„±ì´ ë†’ìŒ)
        st.error(f"ìŠ¤í¬ë˜í•‘ ì¤‘ ì˜¤ë¥˜ ë°œìƒ. (HTML êµ¬ì¡° ë³€ê²½ ê°€ëŠ¥ì„±): {e}")
        return None


def display_weather_scraped(city_name, data):
    """
    Streamlitì— ìŠ¤í¬ë˜í•‘ëœ ë‚ ì”¨ ì •ë³´ë¥¼ í‘œì‹œí•˜ëŠ” í•¨ìˆ˜
    """
    st.subheader(f"âœ¨ {city_name} ë‚ ì”¨")
    
    if data:
        st.write(f"**í˜„ì¬ ì‹œê°:** {data['current_time_desc']}")
        st.write(f"**í˜„ì¬ ì˜¨ë„:** {data['temperature']}")
        st.write(f"**ë‚ ì”¨ ìƒíƒœ:** {data['status']}")
        
        if data['details']:
            st.markdown("**ì„¸ë¶€ ì •ë³´:**")
            col1, col2, col3 = st.columns(3)
            # ì„¸ë¶€ ì •ë³´ë¥¼ 3ì—´ë¡œ ë‚˜ëˆ„ì–´ í‘œì‹œ
            detail_list = list(data['details'].items())
            
            if len(detail_list) > 0: col1.write(f"**{detail_list[0][0]}:** {detail_list[0][1]}")
            if len(detail_list) > 1: col2.write(f"**{detail_list[1][0]}:** {detail_list[1][1]}")
            if len(detail_list) > 2: col3.write(f"**{detail_list[2][0]}:** {detail_list[2][1]}")

    else:
        st.warning(f"**{city_name}**ì˜ ë‚ ì”¨ ì •ë³´ë¥¼ ê°€ì ¸ì˜¤ëŠ” ë° ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤.")
    
    st.markdown("---")


# Streamlit ì•± êµ¬ì„±
st.title("ğŸ‡ªğŸ‡¬ ì´ì§‘íŠ¸ ì£¼ìš” ë„ì‹œ ë‚ ì”¨ ì •ë³´ (ìŠ¤í¬ë˜í•‘)")
st.caption("ğŸš¨ ì´ ì•±ì€ Google ê²€ìƒ‰ ê²°ê³¼ë¥¼ ìŠ¤í¬ë˜í•‘í•©ë‹ˆë‹¤. ì›¹ì‚¬ì´íŠ¸ êµ¬ì¡° ë³€ê²½ ì‹œ ì‘ë™ì´ ë©ˆì¶œ ìˆ˜ ìˆìŠµë‹ˆë‹¤.")
st.markdown("---")

# ëª¨ë“  ë„ì‹œì˜ ë‚ ì”¨ ì •ë³´ í‘œì‹œ
for city_query in EGYPT_CITIES:
    data = get_weather_data_from_google(city_query)
    display_weather_scraped(city_query.split(' ')[0], data) # ë„ì‹œ ì´ë¦„ë§Œ í‘œì‹œ

st.caption("ë°ì´í„° ì¶œì²˜: Google ê²€ìƒ‰ ê²°ê³¼ ìŠ¤í¬ë˜í•‘")
